{
  "run": {
    "id": "run_example_2024",
    "suite_name": "default",
    "model_ids": ["model_phi3mini", "model_llama32_1b"],
    "config": {
      "iterations": 5,
      "concurrency": 1,
      "timeout": 30000,
      "temperature": 0.7,
      "streaming": true
    },
    "hardware_info": {
      "cpu": {
        "manufacturer": "Intel",
        "brand": "Core i7-12700",
        "cores": 12,
        "physicalCores": 12
      },
      "memory": {
        "total": "32 GB"
      },
      "gpu": {
        "model": "NVIDIA GeForce RTX 3070",
        "vram": "8192 MB"
      },
      "os": {
        "platform": "win32",
        "distro": "Windows 11",
        "release": "10.0.22631",
        "arch": "x64"
      }
    },
    "status": "completed",
    "started_at": 1705680000000,
    "completed_at": 1705681200000
  },
  "results": [
    {
      "id": "result_001",
      "run_id": "run_example_2024",
      "model_id": "model_phi3mini",
      "scenario": "Simple Q&A - Short",
      "tps": 45.3,
      "ttft": 120,
      "tpot": 22.1,
      "gen_tps": 45.25,
      "latency_p50": 890,
      "latency_p95": 1050,
      "latency_p99": 1180,
      "error_rate": 0,
      "timeout_rate": 0,
      "cpu_avg": 35.2,
      "ram_avg": 42.1,
      "gpu_avg": 68.5,
      "total_tokens": 245,
      "total_iterations": 5,
      "successful_iterations": 5
    },
    {
      "id": "result_002",
      "run_id": "run_example_2024",
      "model_id": "model_phi3mini",
      "scenario": "Reasoning - Logic Puzzle",
      "tps": 38.7,
      "ttft": 145,
      "tpot": 25.8,
      "gen_tps": 38.76,
      "latency_p50": 1850,
      "latency_p95": 2100,
      "latency_p99": 2280,
      "error_rate": 0,
      "timeout_rate": 0,
      "cpu_avg": 38.5,
      "ram_avg": 43.8,
      "gpu_avg": 72.3,
      "total_tokens": 687,
      "total_iterations": 5,
      "successful_iterations": 5
    },
    {
      "id": "result_003",
      "run_id": "run_example_2024",
      "model_id": "model_llama32_1b",
      "scenario": "Simple Q&A - Short",
      "tps": 62.1,
      "ttft": 95,
      "tpot": 16.1,
      "gen_tps": 62.11,
      "latency_p50": 650,
      "latency_p95": 780,
      "latency_p99": 850,
      "error_rate": 0,
      "timeout_rate": 0,
      "cpu_avg": 28.3,
      "ram_avg": 38.2,
      "gpu_avg": 52.7,
      "total_tokens": 238,
      "total_iterations": 5,
      "successful_iterations": 5
    },
    {
      "id": "result_004",
      "run_id": "run_example_2024",
      "model_id": "model_llama32_1b",
      "scenario": "Reasoning - Logic Puzzle",
      "tps": 55.4,
      "ttft": 108,
      "tpot": 18.0,
      "gen_tps": 55.56,
      "latency_p50": 1420,
      "latency_p95": 1680,
      "latency_p99": 1820,
      "error_rate": 0,
      "timeout_rate": 0,
      "cpu_avg": 32.1,
      "ram_avg": 39.5,
      "gpu_avg": 58.9,
      "total_tokens": 692,
      "total_iterations": 5,
      "successful_iterations": 5
    }
  ],
  "exported_at": 1705681500000,
  "summary": {
    "description": "This is an example benchmark result showing two models (Phi-3 Mini and Llama 3.2 1B) benchmarked on two scenarios from the default suite. In this example, Llama 3.2 1B demonstrates higher throughput (TPS) and lower latency across both scenarios, while both models maintain zero error rates.",
    "key_findings": [
      "Llama 3.2 1B achieves ~37% higher TPS on simple Q&A (62.1 vs 45.3)",
      "Llama 3.2 1B shows ~27% lower P50 latency on reasoning tasks (1420ms vs 1850ms)",
      "Both models maintain 100% success rate (0% error/timeout)",
      "Llama 3.2 1B uses less GPU resources (52-59% vs 68-72%)",
      "Reasoning scenarios take ~2x longer than simple Q&A for both models"
    ]
  }
}
